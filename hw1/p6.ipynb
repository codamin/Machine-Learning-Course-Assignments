{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from PIL import Image, ImageColor\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import os\n",
    "import numpy as np\n",
    "path = './HW1_Q6_Dataset/Q6_Dataset/Images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image paths\n",
    "samples = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier takes the mean of the red and blue for each pixel and the compares them\n",
    "# if red mean is higher then predicted class is red and if blue mean if higher the class is blue\n",
    "def classifier(s):\n",
    "    img = Image.open(os.path.join(path, s))\n",
    "    img = np.array(img)    \n",
    "    mean_red = np.mean(img[:, :, 0])\n",
    "    mean_blue = np.mean(img[:, :, 2])\n",
    "    if mean_red > mean_blue:\n",
    "        return 'm'\n",
    "    else:\n",
    "        return 'c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for i, s in enumerate(samples):\n",
    "    y_true.append(s[0])\n",
    "    y_pred.append(classifier(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[46, 18],\n",
       "       [ 1, 57]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_mtx = confusion_matrix(y_true, y_pred)\n",
    "confusion_mtx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above:\n",
    "- 46+18 samples are actually chelsea --> 48 of them are predicted correctly and 18 of them are predicted wrongly.\n",
    "\n",
    "- 57 + 1 samples are actually manchester --> 57 of them are predicted correctly and 1 of them are predicted wrongly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_classification = np.sum(np.diag(confusion_mtx))\n",
    "correct_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining metricsmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     chelsea       0.98      0.72      0.83        64\n",
      "  manchester       0.76      0.98      0.86        58\n",
      "\n",
      "    accuracy                           0.84       122\n",
      "   macro avg       0.87      0.85      0.84       122\n",
      "weighted avg       0.87      0.84      0.84       122\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=['chelsea', 'manchester']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above:\n",
    "- Accuracy is 84% it means that each prediction is correct by 84% probability.\n",
    "- Recall for chelsea is 72%. It means that among all real chelsea samples, 72% of them are predicted correctly. Similarly recall for manchester is 98% it means that among all real manchester samples, 98% of them are predicted correctly. \n",
    "- Precision for chelsea is 98% it means that among all chelsea predictions, 98% of them were actually chelsea. Similarly precision for manchester is 76% among all manchester predictions, 76% of them were actually manchester."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining confidence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9787234, 0.24     ],\n",
       "       [0.0212766, 0.76     ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence_matrix = confusion_mtx / np.sum(confusion_mtx, axis=0)\n",
    "confidence_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen above:\n",
    "- If we predict a sample to be chelsea, we have 97% confidence that the prediction is correct and 2% confidence that it is not.\n",
    "- If we predict a sample to be manchester, we have 76% confidence that the prediction is correct and 24% that it is not."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
